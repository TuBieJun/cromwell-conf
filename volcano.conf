include required(classpath("application"))
workflow-options {
  # Directory where to write per workflow logs
  workflow-log-dir: "/wegene-qc-workspace-test/ycy/test/cromwell-workflow-logs"

  # When true, per workflow logs will be deleted after copying
  workflow-log-temporary: false
  "write_to_cache": false,
  "read_from_cache": false,
  final_workflow_outputs_dir: "/wegene-qc-workspace-test/liteng/cromwell/output/"
  use_relative_output_paths:ture
}
backend {
  default = Volcano

  providers {
    Volcano {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {

        root = "/wegene-qc-workspace-test/ycy/test/cromwell-executions"
        dockerRoot = "/cromwell-executions"

        runtime-attributes = """
            Int runtime_minutes = 600
            Int cpu = 1
            String dockerimage = "registry-vpc.cn-shenzhen.aliyuncs.com/wegene/bioinfo_software:latest"
            String queue = "cromwell"
            String memorys = "1G"
            String job_template = "/cromwell/template_job.yaml"
        """
        submit = """
            name=${"`echo "+job_name+"|awk -F '_' '{print tolower($3)}'`"}
            uid=${"`echo $(cat /dev/urandom | od -x | head -1 | awk '{print $2$3\"\"$4$5\"\"$6$7\"\"$8$9}')`"}
            jobName=${"`echo $name-$uid`"}
            execDir=$(cd `dirname $0`; pwd)
            workflowDir=${"`dirname "+cwd+"`"}
            cat ${job_template} > ${script}.yaml
                sed -i "s@JOB_NAME@$jobName@g" ${script}.yaml && \
                sed -i "s@WORKFLOW_DIR@$workflowDir@g" ${script}.yaml && \
                sed -i "s@EXECUTION_DIR@$execDir@g" ${script}.yaml && \
                sed -i "s@VOLCANO_QUEUE@${queue}@g" ${script}.yaml && \
                sed -i "s@CPUS@${cpu}@g" ${script}.yaml && \
                sed -i "s@C_IMAGE@${dockerimage}@g" ${script}.yaml && \
                sed -i "s@registry.cn@registry-vpc.cn@g" ${script}.yaml && \
                sed -i "s@MEMORYS@${memorys}@g" ${script}.yaml && \
            vcctl job run -f ${script}.yaml
        """
        kill = "vcctl job delete -N ${job_id}"
        check-alive = "vcctl job view -N ${job_id} "
        #check-alive = "bash `pwd`/check_job_status.sh ${job_id}"
        job-id-regex = ".*run job (\\S+) successfully.*"
        #exit-code-timeout-seconds = 5

        # `glob-link-command` specifies command used to link glob outputs, by default using hard-links.
        # If filesystem doesn't allow hard-links (e.g., beeGFS), change to soft-links as follows:
         glob-link-command = "ln -sL GLOB_PATTERN GLOB_DIRECTORY"

        filesystems {
          local {
            localization: [
              #"soft-link", "copy"
              "soft-link"
            ]
            docker.allow-soft-links: true
            caching {
                duplication-strategy: [
                  #"soft-link", "copy"
                  "soft-link"
                ]
                # Possible values: file, path, path+modtime
                # "file" will compute an md5 hash of the file content.
                # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
                # in order to allow for the original file path to be hashed.
                # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
                # Default: file
                hashing-strategy: "path+modtime"
            }
          }
        }
      }
    }
 }
}

call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  enabled = false

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = true

  # The maximum number of times Cromwell will attempt to copy cache hits before giving up and running the job.
  #max-failed-copy-attempts = 1000000

  # blacklist-cache {
  #   # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache
  #   # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.
  #   enabled: true
  #
  #   # A blacklist grouping can be specified in workflow options which will inform the blacklister which workflows
  #   # should share a blacklist cache.
  #   groupings {
  #     workflow-option: call-cache-blacklist-group
  #     concurrency: 10000
  #     ttl: 2 hours
  #     size: 1000
  #   }
  #
  #   buckets {
  #     # Guava cache concurrency.
  #     concurrency: 10000
  #     # How long entries in the cache should live from the time of their last access.
  #     ttl: 20 minutes
  #     # Maximum number of entries in the cache.
  #     size: 1000
  #   }
  #
  #   hits {
  #     # Guava cache concurrency.
  #     concurrency: 10000
  #     # How long entries in the cache should live from the time of their last access.
  #     ttl: 20 minutes
  #     # Maximum number of entries in the cache.
  #     size: 100000
  #   }
  #
  # }
}
